\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\setlength\parindent{0pt}


\title{Cross Term Attribution}
\author{Frank Ma}
\date{Initial Creation: Nov 2017 \\ Last Update: Dec 2017}

\begin{document}

\maketitle


\section{Introduction}
Post attribution of profit and loss of a security by its contribution risk factors, there is a residual term left. By nature, this residual term is a compound term co-contributed by the uni-variate risk factors. The residuals are expected to be significant when the co-variances among factors are non-negligible and the shocks of risk factors are significantly large.

To practitioners, the challenge is to further breakdown the cross term into contributing factors. Most feasible attribution methods require subjective judgments at certain stage of the processing as the fundamental issue is to attribute non-linear term with a linear combination of contributing factors.

In this study, we introduce an optimization based cross term attribution approach to curtail subjective judgment. The method first assigns arbitrary weights on cross term for each factor then formulates a cost function which aggregates the squared fractions of attributed cross terms to corresponding contributing sources. The optimal cross term weight assignment is found when the cost function is minimized.


\section{Valuation Decomposition}
Consider an asset price $ V $ as a function of contribution risk factors $ x_i $
\begin{equation}
    V(x_1, x_2, \dots, x_m)
\end{equation}
Taylor's theorem finds the decomposition of changes of asset value to contributing risk factors as the following
\begin{equation}
    \Delta_V = \sum^m{\left\{\frac{\partial V}{\partial x_i} \Delta_{x_i} + \frac{1}{2} \frac{\partial^2 V}{\partial x_i^2} \Delta_{x_i}^2 + \mathcal{O}\left(\Delta_{x_i}^3\right)\right\} + \mathcal{O}\left(\Delta_{x_i} \cdot \Delta_{x_j}\right)}
\end{equation}
Define uni-variate pricing shocks per risk factor as below
\begin{equation}
    \Delta_{V_{x_i}} = \frac{\partial V}{\partial x_i} \Delta_{x_i} + \frac{1}{2} \frac{\partial^2 V}{\partial x_i^2} \Delta_{x_i}^{2} + \mathcal{O}\left(\Delta_{x_i}^{3}\right)
\end{equation}
Then the cross term $ \mathcal{O}\left(\Delta_{x_i} \cdot \Delta_{x_j}\right) $ denoted as $ \varepsilon $ is backed out as the following
\begin{equation}
    \varepsilon = \Delta_V - \sum^m{\Delta_{V_{x_i}}}
\end{equation}


\section{Residual Attribution}
The challenge here is to attribute this none-linear term to each contributing factors $ x_i $ through a linear combination. In this study, we manage to find optimal weights $ \omega_i $ proportional to the cross term $ \varepsilon $ contributed by factor $ x_i $. This attribution is justified through a cost function $ \mathcal{F} $. Note this cost function is normalized by factor shocks $ \Delta_{V_{x_i}} $ across all $ n $ valuation scenarios.
\begin{subequations}
    \begin{align}
        \varepsilon_i^k &= \omega_i \varepsilon^k \\
        \mathcal{F} &= \sum_{k}^{n}{\sum_{i}^{m}\left(\frac{\varepsilon_i^k}{\Delta_{V_{x_i}}^k}\right)^2} \\
        \sum^m{\omega_i} &= 1
    \end{align}
\end{subequations}
The contribution of cross term must be optimized if the total cost is minimized. Noticed this must be a constrained optimization, we embed the weight constrain into the cost function and rearrange the formula to the following.
\begin{equation}
    \mathcal{F} = \sum_{i}^{m-1}{\left(\omega_i^2 \sum_{k}^n{\left(\frac{\varepsilon^k}{\Delta_{V_{x_i}}^k}\right)^2}\right)} + \left(1 - \sum_{i}^{m-1}{\omega_i} \right)^2 \sum_{k}^n{\left(\frac{\varepsilon^k}{\Delta_{V_{x_m}}^k}\right)^2}
\end{equation}
To simplify the above equation, define $ \eta_i $
\begin{equation}
    \eta_i = \sum_{k}^{n}{\left(\frac{\varepsilon^{k}}{\Delta_{V_{x_i}}^{k}}\right)^2}
\end{equation}
The cost function is arranged as below
\begin{equation}
    \mathcal{F} = \sum^{m - 1}{\omega_i^2 \eta_i} + \left(1 - \sum^{m - 1}{\omega_i}\right)^2 \eta_m
\end{equation}
To minimize the cost function, we find the first derivatives of $ \mathcal{F} $ against $ \omega_i $ as below.
\begin{equation}
    \frac{\partial \mathcal{F}}{\partial \omega_i} = 2 \omega_i \eta_i - 2 \left(1 - \sum^{m - 1}{\omega_j}\right)\eta_m
\end{equation}
The optimization results of $ \omega_i $ is found through solving a system of formulas where $ \frac{\partial \mathcal{F}}{\partial \omega_i} \equiv 0 $
\begin{equation}
    \omega_i \eta_i + \sum^{m - 1}{\omega_j} \eta_m = \eta_m, \forall i \in [1, m - 1]
\end{equation}
Concisely, in the matrix expression
\begin{equation}
    \left(\boldsymbol{H} + \eta_m\right) \cdot \boldsymbol{\Omega} = \eta_m \cdot \boldsymbol{1}
\end{equation}
where
\begin{equation}
    \boldsymbol{H} = \left| \begin{matrix} \eta_1 & & & \\ & \eta_2 & & \\ & & \ddots & \\ & & & \eta_{m- 1} \end{matrix} \right |_{(m - 1) \times (m - 1)} \boldsymbol{\Omega} = \left| \begin{matrix} \omega_1 \\ \omega_2 \\ \vdots \\ \omega_{m - 1} \end{matrix} \right|_{(m - 1) \times 1} \boldsymbol{1} = \left| \begin{matrix} 1 \\ 1 \\ \vdots \\ 1 \end{matrix} \right|_{(m - 1) \times 1} \nonumber
\end{equation}
To solve for the optimal weights, we need to invert the diagonal plus a constant matrix $ \boldsymbol{H} + \eta_m $. The Sherman-Morrison formula finds the solution as below
\begin{equation}
    \left( \boldsymbol{H} + \eta_m \right)^{-1} = \boldsymbol{H}^{-1} - \frac{\eta_m \cdot \boldsymbol{H}^{-1} \cdot \boldsymbol{1} \cdot \boldsymbol{1}^T \cdot \boldsymbol{H}^{-1}}{1 + \eta_m \cdot \boldsymbol{1}^{T} \cdot \boldsymbol{H}^{-1} \cdot \boldsymbol{1}}
\end{equation}
Then the weights are in the form
\begin{equation}
    \boldsymbol{\Omega} = \boldsymbol{H}^{-1} \cdot \boldsymbol{1} \cdot \eta_m - \frac{\eta_m}{1 + \eta_m \cdot \boldsymbol{1}^{T} \cdot \boldsymbol{H}^{-1} \cdot \boldsymbol{1}} \cdot \boldsymbol{H}^{-1} \cdot \boldsymbol{1} \cdot \boldsymbol{1}^{T} \cdot \boldsymbol{H}^{-1} \cdot \boldsymbol{1} \cdot \eta_m
\end{equation}
Work out each component separately in details to find the following
\begin{subequations}
    \begin{align}
        \boldsymbol{H}^{-1} \cdot \boldsymbol{1} \cdot \eta_m &= \left| \begin{matrix} \frac{1}{\eta_1} \\ \frac{1}{\eta_2} \\ \vdots \\ \frac{1}{\eta_{m - 1}} \end{matrix}\right| \cdot \eta_m \\
        \frac{\eta_m}{1 + \eta_m \cdot \boldsymbol{1}^{T} \cdot \boldsymbol{H}^{-1} \cdot \boldsymbol{1}} &= \frac{1}{\frac{1}{\eta_m} + \sum^{m - 1}{\frac{1}{\eta_i}}} \nonumber \\
        &= \frac{1}{\sum^{m}{\frac{1}{\eta_i}}} \\
        \boldsymbol{H}^{-1} \cdot \boldsymbol{1} \cdot \boldsymbol{1}^{T} \cdot \boldsymbol{H}^{-1} \cdot \boldsymbol{1} \cdot \eta_m &= \left| \begin{matrix} \frac{1}{\eta_1^2} + \frac{1}{\eta_1 \eta_2} + \dots + \frac{1}{\eta_1 \eta_{m - 1}} \\ \frac{1}{\eta_2 \eta_1} + \frac{1}{\eta_{2}^{2} +} \dots + \frac{1}{\eta_2 \eta_{m - 1}}  \\ \vdots \\ \frac{1}{\eta_{m - 1} \eta_1} + \frac{1}{\eta_{m - 1} \eta_2} + \dots + \frac{1}{\eta_{m - 1}^{2}} \end{matrix} \right| \cdot \eta_m \nonumber \\
        &= \left| \begin{matrix}\frac{1}{\eta_1} \left(\sum^{m - 1}{\frac{1}{\eta_i}}\right) \\ \frac{1}{\eta_2} \left(\sum^{m - 1}{\frac{1}{\eta_i}}\right) \\ \vdots \\ \frac{1}{\eta_{m - 1}} \left(\sum^{m - 1}{\frac{1}{\eta_i}}\right) \end{matrix} \right| \cdot \eta_m \nonumber \\
        &= \left| \begin{matrix} \frac{1}{\eta_1} \\ \frac{1}{\eta_2} \\ \vdots \\ \frac{1}{\eta_{m - 1}} \end{matrix} \right| \cdot \sum^{m - 1}{\frac{1}{\eta_i}} \cdot \eta_m
    \end{align}
\end{subequations}
Exam all components together to work out the weights as the following
\begin{align}
    \boldsymbol{\Omega} &= \left| \begin{matrix} \frac{1}{\eta_1} \\ \frac{1}{\eta_2} \\ \vdots \\ \frac{1}{\eta_{m - 1}} \end{matrix} \right| \cdot \eta_m \cdot \left(1 - \frac{\sum^{m - 1}{\frac{1}{\eta_i}}}{\sum^{m}{\frac{1}{\eta_i}}} \right) \nonumber \\
    &= \left| \begin{matrix} \frac{1}{\eta_1} \\ \frac{1}{\eta_2} \\ \vdots \\ \frac{1}{\eta_{m - 1}} \end{matrix} \right| \cdot \frac{1}{\sum^{m}{\frac{1}{\eta_i}}}
\end{align}
To find out the embedded weight $ \omega_m $, we revisit the weight constrain and find
\begin{align}
    \omega_m &= 1 - \frac{\sum^{m - 1}{\frac{1}{\eta_i}}}{\sum^{m}{\frac{1}{\eta_i}}} \nonumber \\
    &= \frac{1}{\eta_m} \cdot \frac{1}{\sum^m{\frac{1}{\eta_i}}}
\end{align}
Observed a generic form of weights in above solution holds for each $ \omega_i $ as
\begin{equation}
    \omega_i = \frac{\frac{1}{\eta_i}}{\sum_{j}^{m}{\frac{1}{\eta_j}}}
\end{equation}
The full spectrum of weights $ \boldsymbol{\Omega}_{m \times 1} $ is formulated as
\begin{subequations}
    \begin{align}
        \boldsymbol{\Omega}_{m \times 1} &= \left| \begin{matrix} \frac{1}{\eta_1} \\ \frac{1}{\eta_2} \\ \vdots \\ \frac{1}{\eta_{m - 1}} \\ \frac{1}{\eta_m} \end{matrix} \right|_{m \times 1} \cdot \sum^m{\frac{1}{\eta_i}} \\
        \eta_i &= \sum_{k}^{n}{\left(\frac{\Delta_V^k - \sum_j^m{\Delta_{V_{x_j}}^{k}}}{\Delta_{V_{x_i}}^{k}}\right)^2}
    \end{align}
\end{subequations}


\section{Computation Complexity}
This method of cross term attribution has three computation intensive parts. The first part is to work out the $ \eta $ by iterating through all scenarios and factors. The process has a computational complexity of $ \boldsymbol{O}(m \times n) $. The second one is to find the weights $ \boldsymbol{\Omega} $. The algorithm itself is a linear process with a computational complexity of $ \boldsymbol{O}(m) $. The last part is to attribute cross term to each factor per scenario. This work has a computational complexity of $ \boldsymbol{O}(m \times n) $. Therefore, the overall process has a computation complexity of $ \boldsymbol{O}(m \times n) $.

\end{document}